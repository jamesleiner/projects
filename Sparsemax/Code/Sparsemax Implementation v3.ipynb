{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sparsemax Regression Implementation\n",
    "\n",
    "We will attempt to replicate the procedures described in section 4.2 of \"From Softmax to Sparsemax\" (Martins, Astudillo) using the dataset birds.arff. These files can be found at: http://mulan.sourceforge.net/datasets-mlc.html\n",
    "\n",
    "In particular, we will try to classify a bird as one of 19 mutually non-exlcusive categories. This will be done using three different loss functions: sparsemax, softmax and logistic (sigmoid). In all cases, the optimization will be performed using the  L-BFGS algorithm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Projecting onto simplex\n",
    "\n",
    "First, we will define the sparsemax evaluation function, which takes vectors of real numbers and computes a probability vector by projecting onto the K-1 dimensional simplex. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "This function takes a vector of real numbers and projects onto the K-1 dimensional simplex. Implemntation/logic was doublechecked against\n",
    "an alternative implementation found at: https://github.com/satyammittal/SMAI-Project-Sparsemax/blob/master/code/train_multilabel_classifier.py\n",
    "\"\"\"\n",
    "def sparsemax_eval(a, radius=1.0):\n",
    "    x0 = a.copy()\n",
    "    d = len(x0);\n",
    "    ind_sort = np.argsort(-x0) # compute order statistics\n",
    "    y0 = x0[ind_sort] #sort\n",
    "    ycum = np.cumsum(y0) \n",
    "    val = (ycum - radius)/np.arange(1,d+1) # compute critical values\n",
    "    ind = np.nonzero(y0 > val)[0]\n",
    "    \n",
    "    # This edecase deals with the case where large floating point numbers cause (k - (k-1)) to evaluate to the same number. \n",
    "    # In this case, we set the largest component to 1 and the remaining components to 0. \n",
    "    if len(ind) == 0:\n",
    "        rho = 0\n",
    "        tau = val[rho]\n",
    "        y = y0 - tau\n",
    "        ind = np.nonzero(y < 0)\n",
    "        y[ind] = 0\n",
    "        x = x0.copy()\n",
    "        x[ind_sort] = y\n",
    "        x[ind_sort[0]] = 1\n",
    "    else: \n",
    "        rho = ind[-1]\n",
    "        tau = val[rho]\n",
    "        y = y0 - tau\n",
    "        ind = np.nonzero(y < 0)\n",
    "        y[ind] = 0\n",
    "        x = x0.copy()\n",
    "        x[ind_sort] = y\n",
    "\n",
    "    return x, tau\n",
    "\n",
    "\n",
    "def softmax_eval(x):\n",
    "    return np.exp(x) / np.sum(np.exp(x), axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading dataset\n",
    "\n",
    "We will now load in the dataset birds.arff\n",
    "\n",
    "The file is split into a seperate training and test dataset. The dataset contains 19 columns correspondign to the response vector and 260 potential features to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "audio-ssd1\n",
      "audio-ssd2\n",
      "audio-ssd3\n",
      "audio-ssd4\n",
      "audio-ssd5\n",
      "audio-ssd6\n",
      "audio-ssd7\n",
      "audio-ssd8\n",
      "audio-ssd9\n",
      "audio-ssd10\n",
      "audio-ssd11\n",
      "audio-ssd12\n",
      "audio-ssd13\n",
      "audio-ssd14\n",
      "audio-ssd15\n",
      "audio-ssd16\n",
      "audio-ssd17\n",
      "audio-ssd18\n",
      "audio-ssd19\n",
      "audio-ssd20\n",
      "audio-ssd21\n",
      "audio-ssd22\n",
      "audio-ssd25\n",
      "audio-ssd26\n",
      "audio-ssd27\n",
      "audio-ssd28\n",
      "audio-ssd29\n",
      "audio-ssd30\n",
      "audio-ssd31\n",
      "audio-ssd32\n",
      "audio-ssd33\n",
      "audio-ssd34\n",
      "audio-ssd35\n",
      "audio-ssd36\n",
      "audio-ssd37\n",
      "audio-ssd38\n",
      "audio-ssd39\n",
      "audio-ssd40\n",
      "audio-ssd41\n",
      "audio-ssd42\n",
      "audio-ssd43\n",
      "audio-ssd44\n",
      "audio-ssd45\n",
      "audio-ssd46\n",
      "audio-ssd49\n",
      "audio-ssd50\n",
      "audio-ssd51\n",
      "audio-ssd52\n",
      "audio-ssd53\n",
      "audio-ssd54\n",
      "audio-ssd55\n",
      "audio-ssd56\n",
      "audio-ssd57\n",
      "audio-ssd58\n",
      "audio-ssd59\n",
      "audio-ssd60\n",
      "audio-ssd61\n",
      "audio-ssd62\n",
      "audio-ssd63\n",
      "audio-ssd64\n",
      "audio-ssd65\n",
      "audio-ssd66\n",
      "audio-ssd67\n",
      "audio-ssd68\n",
      "audio-ssd69\n",
      "audio-ssd70\n",
      "audio-ssd73\n",
      "audio-ssd74\n",
      "audio-ssd75\n",
      "audio-ssd76\n",
      "audio-ssd77\n",
      "audio-ssd78\n",
      "audio-ssd79\n",
      "audio-ssd80\n",
      "audio-ssd81\n",
      "audio-ssd82\n",
      "audio-ssd83\n",
      "audio-ssd84\n",
      "audio-ssd85\n",
      "audio-ssd86\n",
      "audio-ssd87\n",
      "audio-ssd88\n",
      "audio-ssd89\n",
      "audio-ssd90\n",
      "audio-ssd91\n",
      "audio-ssd92\n",
      "audio-ssd93\n",
      "audio-ssd94\n",
      "audio-ssd97\n",
      "audio-ssd98\n",
      "audio-ssd99\n",
      "audio-ssd100\n",
      "audio-ssd101\n",
      "audio-ssd102\n",
      "audio-ssd103\n",
      "audio-ssd104\n",
      "audio-ssd105\n",
      "audio-ssd106\n",
      "audio-ssd107\n",
      "audio-ssd108\n",
      "audio-ssd109\n",
      "audio-ssd110\n",
      "audio-ssd111\n",
      "audio-ssd112\n",
      "audio-ssd113\n",
      "audio-ssd114\n",
      "audio-ssd115\n",
      "audio-ssd116\n",
      "audio-ssd117\n",
      "audio-ssd118\n",
      "audio-ssd121\n",
      "audio-ssd122\n",
      "audio-ssd123\n",
      "audio-ssd124\n",
      "audio-ssd125\n",
      "audio-ssd126\n",
      "audio-ssd127\n",
      "audio-ssd128\n",
      "audio-ssd129\n",
      "audio-ssd130\n",
      "audio-ssd131\n",
      "audio-ssd132\n",
      "audio-ssd133\n",
      "audio-ssd134\n",
      "audio-ssd135\n",
      "audio-ssd136\n",
      "audio-ssd137\n",
      "audio-ssd138\n",
      "audio-ssd139\n",
      "audio-ssd140\n",
      "audio-ssd141\n",
      "audio-ssd145\n",
      "audio-ssd146\n",
      "audio-ssd147\n",
      "audio-ssd148\n",
      "audio-ssd149\n",
      "audio-ssd150\n",
      "audio-ssd151\n",
      "audio-ssd152\n",
      "audio-ssd153\n",
      "audio-ssd154\n",
      "audio-ssd155\n",
      "audio-ssd156\n",
      "audio-ssd157\n",
      "audio-ssd158\n",
      "audio-ssd159\n",
      "audio-ssd160\n",
      "audio-ssd161\n",
      "audio-ssd162\n",
      "audio-ssd163\n",
      "audio-ssd164\n",
      "audio-ssd165\n",
      "audio-ssd166\n",
      "cluster1\n",
      "cluster2\n",
      "cluster3\n",
      "cluster4\n",
      "cluster5\n",
      "cluster6\n",
      "cluster7\n",
      "cluster8\n",
      "cluster9\n",
      "cluster10\n",
      "cluster11\n",
      "cluster12\n",
      "cluster13\n",
      "cluster14\n",
      "cluster15\n",
      "cluster16\n",
      "cluster17\n",
      "cluster18\n",
      "cluster19\n",
      "cluster20\n",
      "cluster21\n",
      "cluster22\n",
      "cluster23\n",
      "cluster24\n",
      "cluster25\n",
      "cluster26\n",
      "cluster27\n",
      "cluster28\n",
      "cluster29\n",
      "cluster30\n",
      "cluster31\n",
      "cluster32\n",
      "cluster33\n",
      "cluster34\n",
      "cluster35\n",
      "cluster36\n",
      "cluster37\n",
      "cluster38\n",
      "cluster39\n",
      "cluster40\n",
      "cluster41\n",
      "cluster42\n",
      "cluster43\n",
      "cluster44\n",
      "cluster45\n",
      "cluster46\n",
      "cluster47\n",
      "cluster48\n",
      "cluster49\n",
      "cluster50\n",
      "cluster51\n",
      "cluster52\n",
      "cluster53\n",
      "cluster54\n",
      "cluster55\n",
      "cluster56\n",
      "cluster57\n",
      "cluster59\n",
      "cluster60\n",
      "cluster61\n",
      "cluster62\n",
      "cluster63\n",
      "cluster64\n",
      "cluster65\n",
      "cluster66\n",
      "cluster67\n",
      "cluster68\n",
      "cluster69\n",
      "cluster70\n",
      "cluster71\n",
      "cluster72\n",
      "cluster73\n",
      "cluster74\n",
      "cluster75\n",
      "cluster76\n",
      "cluster78\n",
      "cluster79\n",
      "cluster80\n",
      "cluster81\n",
      "cluster82\n",
      "cluster83\n",
      "cluster84\n",
      "cluster85\n",
      "cluster86\n",
      "cluster87\n",
      "cluster88\n",
      "cluster89\n",
      "cluster90\n",
      "cluster91\n",
      "cluster92\n",
      "cluster93\n",
      "cluster94\n",
      "cluster95\n",
      "cluster96\n",
      "cluster97\n",
      "cluster98\n",
      "cluster99\n",
      "cluster100\n",
      "segments\n",
      "mean_rect_width\n",
      "std_rect_width\n",
      "mean_rect_height\n",
      "std_rect_height\n",
      "mean_rect_volume\n",
      "std_rect_volume\n",
      "hasSegments\n",
      "location\n",
      "Brown Creeper\n",
      "Pacific Wren\n",
      "Pacific-slope Flycatcher\n",
      "Red-breasted Nuthatch\n",
      "Dark-eyed Junco\n",
      "Olive-sided Flycatcher\n",
      "Hermit Thrush\n",
      "Chestnut-backed Chickadee\n",
      "Varied Thrush\n",
      "Hermit Warbler\n",
      "Swainson\\'s Thrush\n",
      "Hammond\\'s Flycatcher\n",
      "Western Tanager\n",
      "Black-headed Grosbeak\n",
      "Golden Crowned Kinglet\n",
      "Warbling Vireo\n",
      "MacGillivray\\'s Warbler\n",
      "Stellar\\'s Jay\n",
      "Common Nighthawk\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>audio-ssd1</th>\n",
       "      <th>audio-ssd2</th>\n",
       "      <th>audio-ssd3</th>\n",
       "      <th>audio-ssd4</th>\n",
       "      <th>audio-ssd5</th>\n",
       "      <th>audio-ssd6</th>\n",
       "      <th>audio-ssd7</th>\n",
       "      <th>audio-ssd8</th>\n",
       "      <th>audio-ssd9</th>\n",
       "      <th>audio-ssd10</th>\n",
       "      <th>...</th>\n",
       "      <th>Hermit Warbler</th>\n",
       "      <th>Swainson\\'s Thrush</th>\n",
       "      <th>Hammond\\'s Flycatcher</th>\n",
       "      <th>Western Tanager</th>\n",
       "      <th>Black-headed Grosbeak</th>\n",
       "      <th>Golden Crowned Kinglet</th>\n",
       "      <th>Warbling Vireo</th>\n",
       "      <th>MacGillivray\\'s Warbler</th>\n",
       "      <th>Stellar\\'s Jay</th>\n",
       "      <th>Common Nighthawk</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.132445</td>\n",
       "      <td>0.143931</td>\n",
       "      <td>0.227729</td>\n",
       "      <td>0.298556</td>\n",
       "      <td>0.385907</td>\n",
       "      <td>0.378363</td>\n",
       "      <td>0.354708</td>\n",
       "      <td>0.384165</td>\n",
       "      <td>0.360092</td>\n",
       "      <td>0.347465</td>\n",
       "      <td>...</td>\n",
       "      <td>b'0'</td>\n",
       "      <td>b'0'</td>\n",
       "      <td>b'0'</td>\n",
       "      <td>b'0'</td>\n",
       "      <td>b'0'</td>\n",
       "      <td>b'0'</td>\n",
       "      <td>b'0'</td>\n",
       "      <td>b'0'</td>\n",
       "      <td>b'0'</td>\n",
       "      <td>b'0'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.101617</td>\n",
       "      <td>0.130342</td>\n",
       "      <td>0.228117</td>\n",
       "      <td>0.281017</td>\n",
       "      <td>0.365804</td>\n",
       "      <td>0.370122</td>\n",
       "      <td>0.359235</td>\n",
       "      <td>0.388608</td>\n",
       "      <td>0.362013</td>\n",
       "      <td>0.348229</td>\n",
       "      <td>...</td>\n",
       "      <td>b'0'</td>\n",
       "      <td>b'0'</td>\n",
       "      <td>b'1'</td>\n",
       "      <td>b'0'</td>\n",
       "      <td>b'0'</td>\n",
       "      <td>b'0'</td>\n",
       "      <td>b'0'</td>\n",
       "      <td>b'0'</td>\n",
       "      <td>b'0'</td>\n",
       "      <td>b'0'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.005148</td>\n",
       "      <td>0.017877</td>\n",
       "      <td>0.042137</td>\n",
       "      <td>0.062124</td>\n",
       "      <td>0.097340</td>\n",
       "      <td>0.088305</td>\n",
       "      <td>0.084337</td>\n",
       "      <td>0.083204</td>\n",
       "      <td>0.074532</td>\n",
       "      <td>0.071497</td>\n",
       "      <td>...</td>\n",
       "      <td>b'0'</td>\n",
       "      <td>b'1'</td>\n",
       "      <td>b'1'</td>\n",
       "      <td>b'1'</td>\n",
       "      <td>b'0'</td>\n",
       "      <td>b'0'</td>\n",
       "      <td>b'0'</td>\n",
       "      <td>b'0'</td>\n",
       "      <td>b'0'</td>\n",
       "      <td>b'0'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.018792</td>\n",
       "      <td>0.012898</td>\n",
       "      <td>0.027330</td>\n",
       "      <td>0.039521</td>\n",
       "      <td>0.064671</td>\n",
       "      <td>0.068329</td>\n",
       "      <td>0.065799</td>\n",
       "      <td>0.059891</td>\n",
       "      <td>0.048287</td>\n",
       "      <td>0.047820</td>\n",
       "      <td>...</td>\n",
       "      <td>b'0'</td>\n",
       "      <td>b'0'</td>\n",
       "      <td>b'0'</td>\n",
       "      <td>b'0'</td>\n",
       "      <td>b'0'</td>\n",
       "      <td>b'0'</td>\n",
       "      <td>b'0'</td>\n",
       "      <td>b'0'</td>\n",
       "      <td>b'0'</td>\n",
       "      <td>b'0'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.007008</td>\n",
       "      <td>0.014610</td>\n",
       "      <td>0.033637</td>\n",
       "      <td>0.042604</td>\n",
       "      <td>0.065649</td>\n",
       "      <td>0.065047</td>\n",
       "      <td>0.064553</td>\n",
       "      <td>0.058155</td>\n",
       "      <td>0.048516</td>\n",
       "      <td>0.047021</td>\n",
       "      <td>...</td>\n",
       "      <td>b'0'</td>\n",
       "      <td>b'0'</td>\n",
       "      <td>b'0'</td>\n",
       "      <td>b'0'</td>\n",
       "      <td>b'0'</td>\n",
       "      <td>b'0'</td>\n",
       "      <td>b'0'</td>\n",
       "      <td>b'0'</td>\n",
       "      <td>b'0'</td>\n",
       "      <td>b'0'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>318</td>\n",
       "      <td>0.065968</td>\n",
       "      <td>0.005699</td>\n",
       "      <td>0.009809</td>\n",
       "      <td>0.014150</td>\n",
       "      <td>0.027981</td>\n",
       "      <td>0.027554</td>\n",
       "      <td>0.028538</td>\n",
       "      <td>0.031886</td>\n",
       "      <td>0.027338</td>\n",
       "      <td>0.030704</td>\n",
       "      <td>...</td>\n",
       "      <td>b'0'</td>\n",
       "      <td>b'0'</td>\n",
       "      <td>b'0'</td>\n",
       "      <td>b'0'</td>\n",
       "      <td>b'0'</td>\n",
       "      <td>b'0'</td>\n",
       "      <td>b'0'</td>\n",
       "      <td>b'0'</td>\n",
       "      <td>b'0'</td>\n",
       "      <td>b'0'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>319</td>\n",
       "      <td>0.037432</td>\n",
       "      <td>0.010440</td>\n",
       "      <td>0.021009</td>\n",
       "      <td>0.025018</td>\n",
       "      <td>0.089126</td>\n",
       "      <td>0.037404</td>\n",
       "      <td>0.037024</td>\n",
       "      <td>0.046730</td>\n",
       "      <td>0.033445</td>\n",
       "      <td>0.036546</td>\n",
       "      <td>...</td>\n",
       "      <td>b'0'</td>\n",
       "      <td>b'0'</td>\n",
       "      <td>b'0'</td>\n",
       "      <td>b'0'</td>\n",
       "      <td>b'0'</td>\n",
       "      <td>b'0'</td>\n",
       "      <td>b'0'</td>\n",
       "      <td>b'0'</td>\n",
       "      <td>b'0'</td>\n",
       "      <td>b'0'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.200058</td>\n",
       "      <td>0.054787</td>\n",
       "      <td>0.137048</td>\n",
       "      <td>0.162441</td>\n",
       "      <td>0.192939</td>\n",
       "      <td>0.177832</td>\n",
       "      <td>0.178606</td>\n",
       "      <td>0.174532</td>\n",
       "      <td>0.136221</td>\n",
       "      <td>0.142075</td>\n",
       "      <td>...</td>\n",
       "      <td>b'0'</td>\n",
       "      <td>b'0'</td>\n",
       "      <td>b'0'</td>\n",
       "      <td>b'0'</td>\n",
       "      <td>b'0'</td>\n",
       "      <td>b'0'</td>\n",
       "      <td>b'0'</td>\n",
       "      <td>b'0'</td>\n",
       "      <td>b'0'</td>\n",
       "      <td>b'0'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>321</td>\n",
       "      <td>0.064331</td>\n",
       "      <td>0.012261</td>\n",
       "      <td>0.022449</td>\n",
       "      <td>0.026526</td>\n",
       "      <td>0.044141</td>\n",
       "      <td>0.040997</td>\n",
       "      <td>0.039509</td>\n",
       "      <td>0.044909</td>\n",
       "      <td>0.040044</td>\n",
       "      <td>0.041884</td>\n",
       "      <td>...</td>\n",
       "      <td>b'0'</td>\n",
       "      <td>b'0'</td>\n",
       "      <td>b'0'</td>\n",
       "      <td>b'0'</td>\n",
       "      <td>b'0'</td>\n",
       "      <td>b'0'</td>\n",
       "      <td>b'0'</td>\n",
       "      <td>b'0'</td>\n",
       "      <td>b'0'</td>\n",
       "      <td>b'0'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>322</td>\n",
       "      <td>0.008697</td>\n",
       "      <td>0.012031</td>\n",
       "      <td>0.021212</td>\n",
       "      <td>0.028663</td>\n",
       "      <td>0.044081</td>\n",
       "      <td>0.041791</td>\n",
       "      <td>0.044002</td>\n",
       "      <td>0.046693</td>\n",
       "      <td>0.040220</td>\n",
       "      <td>0.041678</td>\n",
       "      <td>...</td>\n",
       "      <td>b'0'</td>\n",
       "      <td>b'0'</td>\n",
       "      <td>b'0'</td>\n",
       "      <td>b'0'</td>\n",
       "      <td>b'0'</td>\n",
       "      <td>b'0'</td>\n",
       "      <td>b'0'</td>\n",
       "      <td>b'0'</td>\n",
       "      <td>b'0'</td>\n",
       "      <td>b'0'</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>323 rows Ã— 279 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     audio-ssd1  audio-ssd2  audio-ssd3  audio-ssd4  audio-ssd5  audio-ssd6  \\\n",
       "0      0.132445    0.143931    0.227729    0.298556    0.385907    0.378363   \n",
       "1      0.101617    0.130342    0.228117    0.281017    0.365804    0.370122   \n",
       "2      0.005148    0.017877    0.042137    0.062124    0.097340    0.088305   \n",
       "3      0.018792    0.012898    0.027330    0.039521    0.064671    0.068329   \n",
       "4      0.007008    0.014610    0.033637    0.042604    0.065649    0.065047   \n",
       "..          ...         ...         ...         ...         ...         ...   \n",
       "318    0.065968    0.005699    0.009809    0.014150    0.027981    0.027554   \n",
       "319    0.037432    0.010440    0.021009    0.025018    0.089126    0.037404   \n",
       "320    0.200058    0.054787    0.137048    0.162441    0.192939    0.177832   \n",
       "321    0.064331    0.012261    0.022449    0.026526    0.044141    0.040997   \n",
       "322    0.008697    0.012031    0.021212    0.028663    0.044081    0.041791   \n",
       "\n",
       "     audio-ssd7  audio-ssd8  audio-ssd9  audio-ssd10  ...  Hermit Warbler  \\\n",
       "0      0.354708    0.384165    0.360092     0.347465  ...            b'0'   \n",
       "1      0.359235    0.388608    0.362013     0.348229  ...            b'0'   \n",
       "2      0.084337    0.083204    0.074532     0.071497  ...            b'0'   \n",
       "3      0.065799    0.059891    0.048287     0.047820  ...            b'0'   \n",
       "4      0.064553    0.058155    0.048516     0.047021  ...            b'0'   \n",
       "..          ...         ...         ...          ...  ...             ...   \n",
       "318    0.028538    0.031886    0.027338     0.030704  ...            b'0'   \n",
       "319    0.037024    0.046730    0.033445     0.036546  ...            b'0'   \n",
       "320    0.178606    0.174532    0.136221     0.142075  ...            b'0'   \n",
       "321    0.039509    0.044909    0.040044     0.041884  ...            b'0'   \n",
       "322    0.044002    0.046693    0.040220     0.041678  ...            b'0'   \n",
       "\n",
       "     Swainson\\'s Thrush  Hammond\\'s Flycatcher  Western Tanager  \\\n",
       "0                  b'0'                   b'0'             b'0'   \n",
       "1                  b'0'                   b'1'             b'0'   \n",
       "2                  b'1'                   b'1'             b'1'   \n",
       "3                  b'0'                   b'0'             b'0'   \n",
       "4                  b'0'                   b'0'             b'0'   \n",
       "..                  ...                    ...              ...   \n",
       "318                b'0'                   b'0'             b'0'   \n",
       "319                b'0'                   b'0'             b'0'   \n",
       "320                b'0'                   b'0'             b'0'   \n",
       "321                b'0'                   b'0'             b'0'   \n",
       "322                b'0'                   b'0'             b'0'   \n",
       "\n",
       "     Black-headed Grosbeak  Golden Crowned Kinglet  Warbling Vireo  \\\n",
       "0                     b'0'                    b'0'            b'0'   \n",
       "1                     b'0'                    b'0'            b'0'   \n",
       "2                     b'0'                    b'0'            b'0'   \n",
       "3                     b'0'                    b'0'            b'0'   \n",
       "4                     b'0'                    b'0'            b'0'   \n",
       "..                     ...                     ...             ...   \n",
       "318                   b'0'                    b'0'            b'0'   \n",
       "319                   b'0'                    b'0'            b'0'   \n",
       "320                   b'0'                    b'0'            b'0'   \n",
       "321                   b'0'                    b'0'            b'0'   \n",
       "322                   b'0'                    b'0'            b'0'   \n",
       "\n",
       "     MacGillivray\\'s Warbler  Stellar\\'s Jay  Common Nighthawk  \n",
       "0                       b'0'            b'0'              b'0'  \n",
       "1                       b'0'            b'0'              b'0'  \n",
       "2                       b'0'            b'0'              b'0'  \n",
       "3                       b'0'            b'0'              b'0'  \n",
       "4                       b'0'            b'0'              b'0'  \n",
       "..                       ...             ...               ...  \n",
       "318                     b'0'            b'0'              b'0'  \n",
       "319                     b'0'            b'0'              b'0'  \n",
       "320                     b'0'            b'0'              b'0'  \n",
       "321                     b'0'            b'0'              b'0'  \n",
       "322                     b'0'            b'0'              b'0'  \n",
       "\n",
       "[323 rows x 279 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import scipy.optimize as opt\n",
    "import time,random\n",
    "import sys,os\n",
    "import pdb\n",
    "from scipy.io import arff\n",
    "import pandas as pd\n",
    "import warnings\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn import metrics\n",
    "from past.builtins import xrange\n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "data = arff.loadarff('birds-train.arff')\n",
    "train = pd.DataFrame(data[0])\n",
    "\n",
    "data = arff.loadarff('birds-test.arff')\n",
    "test = pd.DataFrame(data[0])\n",
    "\n",
    "for col in test.columns: \n",
    "    print(col) \n",
    "\n",
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic data manipulations\n",
    "\n",
    "We will perform some basic cleaning operations to remain consistent with the paper's procedure, including:\n",
    "- Standardize predictor variables to zero mean and unit variance\n",
    "- Remove rows with all 0s as the response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#function to standardize variables to zero mean and unit variance\n",
    "def normalize(xc):\n",
    "    ds = xc.copy()\n",
    "    \n",
    "    #calculate mean and variance\n",
    "    ds_mean = ds.mean(axis=0)\n",
    "    ds_std = ds.std(axis=0)\n",
    "\n",
    "    #apply transformation \n",
    "    for i, features in enumerate(ds):\n",
    "        ds[i] = features - ds_mean\n",
    "        for j in range(len(features)):\n",
    "            if ds_std[j] != 0:\n",
    "                ds[i][j] / ds_std[j]\n",
    "    return ds\n",
    "\n",
    "#for illustration's sake, we will only tain model on the first 50 features included in the dataset\n",
    "x=train.iloc[:,0:75].to_numpy()\n",
    "y=train.iloc[:,260:279].to_numpy().astype(int)\n",
    "weights=np.ones(x.shape[1]*y.shape[1])\n",
    "\n",
    "xnew=test.iloc[:,0:75].to_numpy()\n",
    "ynew=test.iloc[:,260:279].to_numpy().astype(int)\n",
    "\n",
    "\n",
    "x= normalize(x)\n",
    "xnew = normalize(xnew)\n",
    "\n",
    "ind_save = np.nonzero(np.sum(y,axis=1) > 0)\n",
    "ind_save_new = np.nonzero(np.sum(ynew,axis=1) > 0)\n",
    "\n",
    "x = x[ind_save]\n",
    "y= y[ind_save]\n",
    "xnew = xnew[ind_save_new]\n",
    "ynew = ynew[ind_save_new]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining functions for computing loss and gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Computes the support (i.e. non-zero) units of a vector. Returns a vector of 0/1s with 1's corresponding to S(z)\n",
    "\"\"\"\n",
    "def compute_support(probs):\n",
    "    ind = probs.nonzero()[0]\n",
    "    supp =  np.zeros_like(probs)\n",
    "    supp[ind] = 1.\n",
    "    return supp\n",
    "\n",
    "\"\"\"\n",
    "Computes loss function as well as the gradient associated with this loss function\n",
    "\n",
    "Takes in input of:\n",
    "- Weight vector corresponding to coefficients for the regression\n",
    "- X array corresponding to covariate matrix\n",
    "- Y array corresponding to the response matrix\n",
    "- Loss function specifying wheter to evaluate using logistic, softmax, or sparsemax loss\n",
    "\n",
    "Returns output of:\n",
    "- Loss function\n",
    "- Vector corresponding to gradient of loss function\n",
    "\n",
    "\"\"\"\n",
    "def compute_loss(w,x,y,loss_fun):\n",
    "    \n",
    "    num = len(x)\n",
    "    range(num)\n",
    "\n",
    "    loss = 0\n",
    "\n",
    "    weights = w.reshape(x.shape[1], y.shape[1])\n",
    "    loss_gradient  = np.zeros_like(weights)\n",
    "    \n",
    "    scores = x.dot(weights)\n",
    "\n",
    "    for i in range(num):\n",
    "        s = scores[i,:].copy()\n",
    "        xeval = x[i,:].copy()\n",
    "        yeval = y[i,:].copy()\n",
    "        \n",
    "        if loss_fun == 'sparsemax':\n",
    "            z, tau = sparsemax_eval(s)\n",
    "            support = compute_support(z)\n",
    "            loss += -s.dot(yeval) +  0.5 * (s**2 - tau**2).dot(support) + 0.5* np.sum(yeval**2)\n",
    "            delta = -yeval + z\n",
    "            loss_gradient = np.add(loss_gradient,np.outer(xeval, delta))\n",
    "        elif loss_fun == 'logistic':\n",
    "            pr = np.exp(s)  / (1 + np.exp(s))\n",
    "            log_pr = np.log(pr)\n",
    "            log_neg_pr = np.log(1-pr)\n",
    "            loss += -log_pr.dot(yeval)  -(1-yeval).dot(log_neg_pr)\n",
    "            delta = pr - yeval\n",
    "            loss_gradient = np.add(loss_gradient,np.outer(xeval, delta))\n",
    "        elif loss_fun == 'softmax':\n",
    "            pr = np.exp(s) / np.sum(np.exp(s))\n",
    "            loss += -s.dot(yeval) + np.log(np.sum(np.exp(s)))\n",
    "            delta = pr - yeval \n",
    "            loss_gradient = np.add(loss_gradient,np.outer(xeval, delta))\n",
    "    \n",
    "    loss /= num\n",
    "    loss_gradient /= num\n",
    "        \n",
    "    return loss, loss_gradient.flatten()\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Takes in coefficients + probability threshold and returns micro-F1 and macro-F1 metrics based on resulting prediction matrix\n",
    "assuming a logistic activation functions\n",
    "\n",
    "Inputs are:\n",
    "- Weight vector of coefficients\n",
    "- X array corresponding to covariate matrix\n",
    "- Y array corresponding to the response matrix\n",
    "- Probability threshold\n",
    "\n",
    "Outputs are:\n",
    "- Matrix of probability outputs\n",
    "- Matrix of prediction outputs\n",
    "- Micro-averaged F1 score\n",
    "- Macro-averaged F1 score\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def score_logistic(w,xt,yt,threshold):\n",
    "    w_score = w.copy()\n",
    "    w_score = w_score.reshape(x.shape[1], y.shape[1])\n",
    "    scores = xt.dot(w_score)\n",
    "    pmatrix = np.exp(scores)/(1+np.exp(scores))\n",
    "    pred = 1*(pmatrix > threshold)\n",
    "\n",
    "    micro_f1 = metrics.f1_score(yt,pred,average='micro')\n",
    "    macro_f1 = metrics.f1_score(yt,pred,average='macro')\n",
    "    return pred, pmatrix, [micro_f1, macro_f1]\n",
    "\n",
    "\"\"\"\n",
    "Takes in coefficients + probability threshold and returns micro-F1 and macro-F1 metrics based on resulting prediction matrix\n",
    "assuming a softmax function\n",
    "\n",
    "Inputs are:\n",
    "- Weight vector of coefficients\n",
    "- X array corresponding to covariate matrix\n",
    "- Y array corresponding to the response matrix\n",
    "- Probability threshold\n",
    "\n",
    "Outputs are:\n",
    "- Matrix of probability outputs\n",
    "- Matrix of prediction outputs\n",
    "- Micro-averaged F1 score\n",
    "- Macro-averaged F1 score\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def score_softmax(w,xt,yt,threshold):\n",
    "    w_score = w.copy()\n",
    "    w_score = w_score.reshape(x.shape[1], y.shape[1])\n",
    "    scores = xt.dot(w_score)\n",
    "    sc = np.exp(scores)\n",
    "    sums = np.sum(sc,axis=1)\n",
    "    pmatrix = sc/sums[:,None]\n",
    "    pred = 1*(pmatrix > threshold)\n",
    "\n",
    "    micro_f1 = metrics.f1_score(yt,pred,average='micro')\n",
    "    macro_f1 = metrics.f1_score(yt,pred,average='macro')\n",
    "    return pred, pmatrix, [micro_f1, macro_f1]\n",
    "\n",
    "\"\"\"\n",
    "Takes in coefficients + probability threshold and returns micro-F1 and macro-F1 metrics based on resulting prediction matrix\n",
    "assuming a sparsemax function\n",
    "\n",
    "Inputs are:\n",
    "- Weight vector of coefficients\n",
    "- X array corresponding to covariate matrix\n",
    "- Y array corresponding to the response matrix\n",
    "- Probability threshold\n",
    "\n",
    "Outputs are:\n",
    "- Matrix of probability outputs\n",
    "- Matrix of prediction outputs\n",
    "- Micro-averaged F1 score\n",
    "- Macro-averaged F1 score\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def score_sparsemax(w,xt,yt,t):\n",
    "    w_score = w.copy()\n",
    "    w_score = w_score.reshape(x.shape[1], y.shape[1])\n",
    "    scores = t*xt.dot(w_score)\n",
    "    pmatrix = scores.copy()\n",
    "    \n",
    "    for i in range(x.shape[1]):\n",
    "        pmatrix[i] = sparsemax_eval(scores[i])[1]\n",
    "\n",
    "    pred = 1*(pmatrix > 0)\n",
    "\n",
    "    micro_f1 = metrics.f1_score(yt,pred,average='micro')\n",
    "    macro_f1 = metrics.f1_score(yt,pred,average='macro')\n",
    "    return pred, pmatrix, [micro_f1, macro_f1]\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper-parameter tuning\n",
    "\n",
    "Tunes probability thresholds for logistic and softmax loss using 5-fols cross validation on the training dataset. \n",
    "\n",
    "Based on the results, I use a threshold of 0.65 for logistic regression. I use a threshold of K = 1/19 as a threshold for softmax regression. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             K  F1 Micro-Average  F1 Macro-Average\n",
      "Threshold                                         \n",
      "0.00       3.0          0.182184          0.171570\n",
      "0.05       3.0          0.194270          0.180325\n",
      "0.10       3.0          0.199350          0.183129\n",
      "0.15       3.0          0.206812          0.188219\n",
      "0.20       3.0          0.210007          0.188839\n",
      "0.25       3.0          0.212609          0.187569\n",
      "0.30       3.0          0.217861          0.191187\n",
      "0.35       3.0          0.216693          0.188962\n",
      "0.40       3.0          0.225281          0.193475\n",
      "0.45       3.0          0.223048          0.189602\n",
      "0.50       3.0          0.225135          0.187934\n",
      "0.55       3.0          0.227616          0.187208\n",
      "0.60       3.0          0.229548          0.187147\n",
      "0.65       3.0          0.231250          0.185564\n",
      "0.70       3.0          0.227753          0.183963\n",
      "0.75       3.0          0.219618          0.174602\n",
      "0.80       3.0          0.223364          0.169409\n",
      "0.85       3.0          0.221981          0.162200\n",
      "0.90       3.0          0.210809          0.151425\n",
      "0.95       3.0          0.173193          0.112347\n",
      "             K  F1 Micro-Average  F1 Macro-Average\n",
      "Threshold                                         \n",
      "0.000000   3.0          0.182184          0.171570\n",
      "0.052632   3.0          0.285815          0.177012\n",
      "0.105263   3.0          0.257467          0.145573\n",
      "0.157895   3.0          0.254360          0.138265\n",
      "0.210526   3.0          0.246228          0.126321\n",
      "0.263158   3.0          0.248302          0.125620\n",
      "0.315789   3.0          0.242181          0.113334\n",
      "0.368421   3.0          0.225083          0.101121\n",
      "0.421053   3.0          0.209102          0.088954\n",
      "0.473684   3.0          0.202420          0.085109\n",
      "0.526316   3.0          0.195161          0.080973\n",
      "0.578947   3.0          0.191802          0.075027\n",
      "0.631579   3.0          0.190413          0.074530\n",
      "0.684211   3.0          0.180112          0.069928\n",
      "0.736842   3.0          0.166198          0.063526\n",
      "0.789474   3.0          0.168729          0.063931\n",
      "0.842105   3.0          0.161731          0.062167\n",
      "0.894737   3.0          0.146940          0.059499\n",
      "0.947368   3.0          0.127566          0.051345\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\"\n",
    "Performs K-folds cross validation\n",
    "\n",
    "Inputs are:\n",
    "- Weight vector of coefficients\n",
    "- X array corresponding to covariate matrix\n",
    "- Y array corresponding to the response matrix\n",
    "- Probability threshold\n",
    "\n",
    "Outputs are:\n",
    "- Matrix of probability outputs\n",
    "- Matrix of prediction outputs\n",
    "- Micro-averaged F1 score\n",
    "- Macro-averaged F1 score\n",
    "\n",
    "\"\"\"\n",
    "def crossval(weights, x,y,num_splits,loss_fun):\n",
    "    kf = KFold(n_splits=num_splits)\n",
    "    kf.get_n_splits(x)\n",
    "    num_cats = y.shape[1]\n",
    "    \n",
    "\n",
    "    i=0\n",
    "    metric_list = []\n",
    "    for train_index, test_index in kf.split(x):\n",
    "        i= i + 1\n",
    "        t = train_index\n",
    "        s = test_index\n",
    "\n",
    "        x_train= x[t]\n",
    "        y_train = y[t]\n",
    "        x_test= x[s]\n",
    "        y_test = y[s]\n",
    "\n",
    "        if loss_fun == 'logistic':\n",
    "            w_k, value, d = opt.fmin_l_bfgs_b(compute_loss,\n",
    "                                x0=weights,\n",
    "                                args=(x_train, y_train, 'logistic'),\n",
    "                                m=10,\n",
    "                                factr=100,\n",
    "                                pgtol=1e-08,\n",
    "                                epsilon=1e-12,\n",
    "                                approx_grad=False,\n",
    "                                disp=True,\n",
    "                                maxfun=1000,\n",
    "                                maxiter=100)\n",
    "            for j in range(20): \n",
    "                a,b,metric = score_logistic(w_k,x_test,y_test,j/20)\n",
    "                metric = np.append([i, j/20], metric)\n",
    "                metric_list.append(metric)\n",
    "        elif loss_fun == 'softmax':\n",
    "            w_k, value, d = opt.fmin_l_bfgs_b(compute_loss,\n",
    "                                x0=weights,\n",
    "                                args=(x_train, y_train, 'softmax'),\n",
    "                                m=10,\n",
    "                                factr=100,\n",
    "                                pgtol=1e-08,\n",
    "                                epsilon=1e-12,\n",
    "                                approx_grad=False,\n",
    "                                disp=True,\n",
    "                                maxfun=1000,\n",
    "                                maxiter=100)\n",
    "            for j in range(num_cats): \n",
    "                a,b,metric = score_softmax(w_k,x_test,y_test,j/num_cats)\n",
    "                metric = np.append([i, j/num_cats], metric)\n",
    "                metric_list.append(metric)\n",
    "    df = pd.DataFrame(metric_list)\n",
    "    df.columns = ['K','Threshold','F1 Micro-Average','F1 Macro-Average']\n",
    "    return df.groupby('Threshold').mean()\n",
    "\n",
    "print(crossval(weights,x,y,5,'logistic'))\n",
    "print(crossval(weights,x,y,5,'softmax'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results for Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Micro F1 Score: 0.21366024518388796\n",
      "Macro F1 Score: 0.17908162169430974\n"
     ]
    }
   ],
   "source": [
    "\n",
    "w_logistic, value, d = \\\n",
    "      opt.fmin_l_bfgs_b(compute_loss,\n",
    "                        x0=weights,\n",
    "                        args=(x, y, 'logistic'),\n",
    "                        m=10,\n",
    "                        factr=100,\n",
    "                        pgtol=1e-08,\n",
    "                        epsilon=1e-12,\n",
    "                        approx_grad=False,\n",
    "                        disp=True,\n",
    "                        maxfun=1000,\n",
    "                        maxiter=100)\n",
    "\n",
    "\n",
    "_,_,logistic_metric = score_logistic(w_logistic,xnew,ynew,0.65)\n",
    "\n",
    "print(\"Micro F1 Score: \" + str(logistic_metric[0]))\n",
    "print(\"Macro F1 Score: \" + str(logistic_metric[1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results for Softmax Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Micro F1 Score: 0.3001686340640809\n",
      "Macro F1 Score: 0.17462059849372655\n"
     ]
    }
   ],
   "source": [
    "w_softmax, value, d2 = \\\n",
    "      opt.fmin_l_bfgs_b(compute_loss,\n",
    "                        x0=weights,\n",
    "                        args=(x, y, 'softmax'),\n",
    "                        m=10,\n",
    "                        factr=100,\n",
    "                        pgtol=1e-08,\n",
    "                        epsilon=1e-12,\n",
    "                        approx_grad=False,\n",
    "                        disp=True,\n",
    "                        maxfun=1000,\n",
    "                        maxiter=10000)\n",
    "\n",
    "\n",
    "_,_,softmax_metric = score_softmax(w_softmax,xnew,ynew,1/19)\n",
    "\n",
    "print(\"Micro F1 Score: \" + str(softmax_metric[0]))\n",
    "print(\"Macro F1 Score: \" + str(softmax_metric[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results for Sparsemax Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Micro F1 Score: 0.21712647796488713\n",
      "Macro F1 Score: 0.2018138672977463\n"
     ]
    }
   ],
   "source": [
    "    w_sparsemax, value, d = \\\n",
    "      opt.fmin_l_bfgs_b(compute_loss,\n",
    "                        x0=weights,\n",
    "                        args=(x, y, 'sparsemax'),\n",
    "                        m=10,\n",
    "                        factr=100,\n",
    "                        pgtol=1e-08,\n",
    "                        epsilon=1e-12,\n",
    "                        approx_grad=False,\n",
    "                        disp=True,\n",
    "                        maxfun=1000,\n",
    "                        maxiter=100)\n",
    "_,_,sparsemax_metric = score_sparsemax(w_sparsemax,x,y,1)\n",
    "\n",
    "print(\"Micro F1 Score: \" + str(sparsemax_metric[0]))\n",
    "print(\"Macro F1 Score: \" + str(sparsemax_metric[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
